

[TOC]



## 计算机网络

### 体系结构

<img src="https://camo.githubusercontent.com/ae61b7eaf557549d449c8658c14fd8917c22242dcdb50325b164896a0d7a709e/68747470733a2f2f67697465652e636f6d2f6875696875742f696e746572766965772f7261772f6d61737465722f696d616765732f2545382541452541312545372541452539372545362539432542412545372542442539312545372542422539432545342542442539332545372542332542422545372542422539332545362539452538342e706e67" alt="计算机网络体系结构" style="zoom: 67%;" />

| 分层       | 作用                                                | 协议                                                |
| ---------- | --------------------------------------------------- | --------------------------------------------------- |
| 物理层     | 通过媒介传输比特，确定机械及电气规范（比特 Bit）    | RJ45、CLOCK、IEEE802.3（中继器，集线器）            |
| 数据链路层 | 将比特组装成帧和点到点的传递（帧 Frame）            | PPP、FR、HDLC、VLAN、MAC（网桥，交换机）            |
| 网络层     | 负责数据包从源到宿的传递和网际互连（包 Packet）     | IP、ICMP、ARP、RARP、OSPF、IPX、RIP、IGRP（路由器） |
| 运输层     | 提供端到端的可靠报文传递和错误恢复（ 段Segment）    | TCP、UDP、SPX                                       |
| 会话层     | 建立、管理和终止会话（会话协议数据单元 SPDU）       | NFS、SQL、NETBIOS、RPC                              |
| 表示层     | 对数据进行翻译、加密和压缩（表示协议数据单元 PPDU） | JPEG、MPEG、ASII                                    |
| 应用层     | 允许访问OSI环境的手段（应用协议数据单元 APDU）      | FTP、DNS、Telnet、SMTP、HTTP、WWW、NFS              |















### TCP/IP

#### TCP

**TCP（Transmission Control Protocol，传输控制协议）**是一种面向连接的、可靠的、基于字节流的传输层通信协议，其传输的单位是报文段。



**TCP 如何保证可靠传输**

- 确认和超时重传
- 数据合理分片和排序
- 流量控制
- 拥塞控制
- 数据校验



**TCP 报文结构**

[![TCP 报文](https://camo.githubusercontent.com/c47b30002614e3d26fd64a02cadb9497bd51d3b8cae015ac58de93ef6d871e0f/68747470733a2f2f67697465652e636f6d2f6875696875742f696e746572766965772f7261772f6d61737465722f696d616765732f5443502545362538412541352545362539362538372e706e67)](https://camo.githubusercontent.com/c47b30002614e3d26fd64a02cadb9497bd51d3b8cae015ac58de93ef6d871e0f/68747470733a2f2f67697465652e636f6d2f6875696875742f696e746572766965772f7261772f6d61737465722f696d616765732f5443502545362538412541352545362539362538372e706e67)

**TCP 首部**

[![TCP 首部](https://camo.githubusercontent.com/c850dd8c5a34fa83b55d76d38847afffc520b69e5ea8d54cadc232092aecb608/68747470733a2f2f67697465652e636f6d2f6875696875742f696e746572766965772f7261772f6d61737465722f696d616765732f5443502545392541362539362545392538332541382e706e67)](https://camo.githubusercontent.com/c850dd8c5a34fa83b55d76d38847afffc520b69e5ea8d54cadc232092aecb608/68747470733a2f2f67697465652e636f6d2f6875696875742f696e746572766965772f7261772f6d61737465722f696d616765732f5443502545392541362539362545392538332541382e706e67)



**TCP 状态控制码（Code，Control Flag）**，占 6 比特，含义如下：

- URG：紧急比特（urgent），当 `URG＝1` 时，表明紧急指针字段有效，代表该封包为紧急封包。它告诉系统此报文段中有紧急数据，应尽快传送(相当于高优先级的数据)， 且上图中的 Urgent Pointer 字段也会被启用。
- ACK：确认比特（Acknowledge）。只有当 `ACK＝1` 时确认号字段才有效，代表这个封包为确认封包。当 `ACK＝0` 时，确认号无效。
- PSH：（Push function）若为 1 时，代表要求对方立即传送缓冲区内的其他对应封包，而无需等缓冲满了才送。
- RST：复位比特(Reset)，当 `RST＝1` 时，表明 TCP 连接中出现严重差错（如由于主机崩溃或其他原因），必须释放连接，然后再重新建立运输连接。
- SYN：同步比特(Synchronous)，SYN 置为 1，就表示这是一个连接请求或连接接受报文，通常带有 SYN 标志的封包表示『主动』要连接到对方的意思。
- FIN：终止比特(Final)，用来释放一个连接。当 `FIN＝1` 时，表明此报文段的发送端的数据已发送完毕，并要求释放运输连接。







#### UDP

**UDP（User Datagram Protocol，用户数据报协议）**是 OSI 模型中一种无连接的传输层协议，提供面向事务的简单不可靠信息传送服务，其传输的单位是用户数据报。







#### TCP 与 UDP 的区别

1. TCP 面向连接，UDP 是无连接的；
2. TCP 提供可靠的服务，也就是说，通过 TCP 连接传送的数据，无差错，不丢失，不重复，且按序到达；UDP 尽最大努力交付，即不保证可靠交付
3. TCP 的逻辑通信信道是全双工的可靠信道；UDP 则是不可靠信道
4. 每一条 TCP 连接只能是点到点的；UDP 支持一对一，一对多，多对一和多对多的交互通信
5. TCP 面向字节流（可能出现黏包问题），实际上是 TCP 把数据看成一连串无结构的字节流；UDP 是面向报文的（不会出现黏包问题）
6. UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如 IP 电话，实时视频会议等）
7. TCP 首部开销20字节；UDP 的首部开销小，只有 8 个字节







#### TCP 三次握手

![img](https://pic4.zhimg.com/80/v2-88770c6a0ead09903a81cd815b4aa70f_1440w.jpg)

**为什么不是两次？**

根本原因: 无法确认客户端的接收能力。

如果是两次，你现在发了 SYN 报文想握手，但是这个包 **滞留** 在了当前的网络中迟迟没有到达，TCP 以为这是丢了包，于是重传，两次握手建立好了连接。看似没有问题，但是连接关闭后，如果这个 **滞留** 在网路中的包到达了服务端呢？这时候由于是两次握手，服务端只要接收到然后发送相应的数据包，就默认 **建立连接**，但是现在客户端已经断开了。看到问题的吧，这就带来了连接资源的浪费。



**三次握手过程中可以携带数据么？**

第三次握手的时候，可以携带。前两次握手不能携带数据。

如果前两次握手能够携带数据，那么一旦有人想攻击服务器，那么他只需要在第一次握手中的 SYN 报文中放大量数据，那么服务器势必会消耗更多的 **时间**和 **内存空间 **去处理这些数据，增大了服务器被攻击的风险。第三次握手的时候，客户端已经处于 `ESTABLISHED`状态，并且已经能够确认服务器的接收、发送能力正常，这个时候相对安全了，可以携带数据。







#### TCP 四次挥手

![img](https://pic3.zhimg.com/80/v2-dbf32d5429baa5180eab507e102df1fe_1440w.jpg)

* 刚开始双方处于 `ESTABLISHED`状态。客户端要断开了，向服务器发送 `FIN` 报文。

* 发送后客户端变成了 `FIN-WAIT-1`状态。注意, 这时候客户端同时也变成了 `half-close(半关闭)`状态，即无法向服务端发送报文，只能接收。服务端接收后向客户端确认，变成了 `CLOSED-WAIT`状态。客户端接收到了服务端的确认，变成了 `FIN-WAIT2`状态。

* 随后，服务端向客户端发送 `FIN`，自己进入 `LAST-ACK`状态，客户端收到服务端发来的 `FIN`后，自己变成了 `TIME-WAIT`状态，然后发送 ACK 给服务端。注意了，这个时候，客户端需要等待足够长的时间，具体来说，是 2 个 `MSL`( `Maximum Segment Lifetime，报文最大生存时间`), 在这段时间内如果客户端没有收到服务端的重发请求，那么表示 ACK 成功到达，挥手结束，否则客户端重发 ACK。



**等待2MSL的意义**

如果不等待会怎样？如果不等待，客户端直接跑路，当服务端还有很多数据包要给客户端发，且还在路上的时候，若客户端的端口此时刚好被新的应用占用，那么就接收到了无用数据包，造成数据包混乱。所以，最保险的做法是等服务器发来的数据包都死翘翘再启动新的应用。那，照这样说一个 MSL 不就不够了吗，为什么要等待 2 MSL?

- 1 个 MSL 确保四次挥手中主动关闭方最后的 ACK 报文最终能达到对端
- 1 个 MSL 确保对端没有收到 ACK 重传的 FIN 报文可以到达







#### TCP 流量控制

对于发送端和接收端而言，TCP 需要把发送的数据放到 **发送缓存区**, 将接收的数据放到 **接收缓存区**。而流量控制索要做的事情，就是在通过接收缓存区的大小，控制发送端的发送。如果对方的接收缓存区满了，就不能再继续发送了。

**发送窗口**

发送端的滑动窗口结构如下:

<img src="https://img-blog.csdnimg.cn/20190518001324584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NpYWl5,size_16,color_FFFFFF,t_70" alt="TCP 协议中的窗口管理_ciaiy的博客-CSDN博客_tcp 协议的窗口" style="zoom: 33%;" />





**接收窗口**

接收端的窗口结构如下:

<img src="https://walepic.oss-cn-hangzhou.aliyuncs.com/tcp/TCP%20%E6%8E%A5%E6%94%B6%E7%AA%97%E5%8F%A3.png" alt="TCP滑动窗口详解-上地信息-shangdixinxi.com" style="zoom: 50%;" />

REV 即 `receive`，NXT 表示下一个接收的位置，WND 表示接收窗口大小。



**流量控制过程**

这里以一个最简单的来回来模拟一下流量控制的过程。

首先双方三次握手，初始化各自的窗口大小，均为 200 个字节。假如当前发送端给接收端发送 100 个字节，那么此时对于发送端而言，SND.NXT 当然要右移 100 个字节，也就是说当前的 `可用窗口`减少了 100 个字节，这很好理解。

现在这 100 个到达了接收端，被放到接收端的缓冲队列中。不过此时由于大量负载的原因，接收端处理不了这么多字节，只能处理 40 个字节，剩下的 `60` 个字节被留在了缓冲队列中

注意了，此时接收端的情况是处理能力不够用啦，你发送端给我少发点，所以此时接收端的接收窗口应该缩小，具体来说，缩小 60 个字节，由 200 个字节变成了 140 字节，因为缓冲队列还有 60 个字节没被应用拿走。因此，接收端会在 ACK 的报文首部带上缩小后的滑动窗口 140 字节，发送端对应地调整发送窗口的大小为 140 个字节。

此时对于发送端而言，已经发送且确认的部分增加 40 字节，也就是 SND.UNA 右移 40 个字节，同时 **发送窗口**缩小为 140 个字节。这也就是 **流量控制**的过程。尽管回合再多，整个控制的过程和原理是一样的。





#### TCP 拥塞控制

**流量控制** 发生在发送端跟接收端之间，并没有考虑到整个网络环境的影响。

如果说当前网络特别差，特别容易丢包，那么发送端就应该注意一些了。而这，也正是 `拥塞控制`需要处理的问题。对于拥塞控制来说，TCP 每条连接都需要维护两个核心状态:

- 拥塞窗口（Congestion Window，cwnd）
- 慢启动阈值（Slow Start Threshold，ssthresh）



**拥塞窗口**

拥塞窗口（Congestion Window，cwnd）是指目前自己还能传输的数据量大小。那么之前介绍了接收窗口的概念，两者有什么区别呢？

- 接收窗口(rwnd)是 `接收端`给的限制
- 拥塞窗口(cwnd)是 `发送端`的限制

限制谁呢？限制的是 `发送窗口`的大小。有了这两个窗口，如何来计算 `发送窗口`？

```text
    发送窗口大小 = min(rwnd, cwnd)
```

取两者的较小值。而拥塞控制，就是来控制 `cwnd`的变化。



**慢启动**

刚开始进入传输数据的时候，你是不知道现在的网路到底是稳定还是拥堵的，如果做的太激进，发包太急，那么疯狂丢包，造成雪崩式的网络灾难。因此，拥塞控制首先就是要采用一种保守的算法来慢慢地适应整个网路，这种算法叫 `慢启动`。运作过程如下:

- 首先，三次握手，双方宣告自己的接收窗口大小
- 双方初始化自己的 **拥塞窗口**(cwnd)大小
- 在开始传输的一段时间，发送端每收到一个 ACK，拥塞窗口大小加 1，也就是说，每经过一个 RTT，cwnd 翻倍。如果说初始窗口为 10，那么第一轮 10 个报文传完且发送端收到 ACK 后，cwnd 变为 20，第二轮变为 40，第三轮变为 80，依次类推。

难道就这么无止境地翻倍下去？当然不可能。它的阈值叫做 **慢启动阈值**，当 cwnd 到达这个阈值之后，好比踩了下刹车，别涨了那么快了，老铁，先 hold 住！在到达阈值后，如何来控制 cwnd 的大小呢？这就是拥塞避免做的事情了。



**拥塞避免**

原来每收到一个 ACK，cwnd 加1，现在到达阈值了，cwnd 只能加这么一点: **1 / cwnd**。那你仔细算算，一轮 RTT 下来，收到 cwnd 个 ACK, 那最后拥塞窗口的大小 cwnd 总共才增加 1。也就是说，以前一个 RTT 下来， `cwnd`翻倍，现在 `cwnd`只是增加 1 而已。当然， **慢启动**和 **拥塞避免**是一起作用的，是一体的。



**快速重传**

在 TCP 传输的过程中，如果发生了丢包，即接收端发现数据段不是按序到达的时候，接收端的处理是重复发送之前的 ACK。比如第 5 个包丢了，即使第 6、7 个包到达的接收端，接收端也一律返回第 4 个包的 ACK。

当发送端收到 3 个重复的 ACK 时，意识到丢包了，于是马上进行重传，不用等到一个 RTO 的时间到了才重传。这就是 **快速重传**，它解决的是 **是否需要重传**的问题。



**选择性重传**

那你可能会问了，既然要重传，那么只重传第 5 个包还是第5、6、7 个包都重传呢？当然第 6、7 个都已经到达了，TCP 的设计者也不傻，已经传过去干嘛还要传？干脆记录一下哪些包到了，哪些没到，针对性地重传。

在收到发送端的报文后，接收端回复一个 ACK 报文，那么在这个报文首部的可选项中，就可以加上 `SACK`这个属性，通过 `left edge`和 `right edge`告知发送端已经收到了哪些区间的数据报。因此，即使第 5 个包丢包了，当收到第 6、7 个包之后，接收端依然会告诉发送端，这两个包到了。

剩下第 5 个包没到，就重传这个包。这个过程也叫做 **选择性重传(SACK，Selective Acknowledgment)**，它解决的是 **如何重传**的问题。



**快速恢复**

当然，发送端收到三次重复 ACK 之后，发现丢包，觉得现在的网络已经有些拥塞了，自己会进入 **快速恢复** 阶段。在这个阶段，发送端如下改变：

- 拥塞阈值降低为 cwnd 的一半
- cwnd 的大小变为拥塞阈值
- cwnd 线性增加







#### TCP 黏包问题

TCP 是一个基于字节流的传输服务（UDP 基于报文的），“流” 意味着 TCP 所传输的数据是没有边界的。所以可能会出现两个数据包黏在一起的情况。

**解决方案** 

- 发送定长包。如果每个消息的大小都是一样的，那么在接收对等方只要累计接收数据，直到数据等于一个定长的数值就将它作为一个消息。
- 包头加上包体长度。包头是定长的 4 个字节，说明了包体的长度。接收对等方先接收包头长度，依据包头长度来接收包体。
- 在数据包之间设置边界，如添加特殊符号 `\r\n` 标记。FTP 协议正是这么做的。但问题在于如果数据正文中也含有 `\r\n`，则会误判为消息的边界。
- 使用更加复杂的应用层协议。







#### TCP keep-alive 机制

TCP 层面也是有 `keep-alive`机制。当有一方因为网络故障或者宕机导致连接失效，由于 TCP 并不是一个轮询的协议，在下一个数据包到达之前，对端对连接失效的情况是一无所知的。这个时候就出现了 keep-alive,  它的作用就是**探测对端的连接有没有失效**。







#### **半连接队列和 SYN Flood 攻击**

三次握手前，服务端的状态从 `CLOSED`变为 `LISTEN`, 同时在内部创建了两个队列： **半连接队列**和 **全连接队列**。

* 当客户端发送 `SYN`到服务端，服务端收到以后回复 `ACK`和 `SYN`，状态由 `LISTEN`变为 `SYN_RCVD`，此时这个连接就被推入了 **SYN队列**，也就是 **半连接队列**。

* 当客户端返回 `ACK`, 服务端接收后，三次握手完成。这个时候连接等待被具体的应用取走，在被取走之前，它会被推入另外一个 TCP 维护的队列，也就是 **全连接队列(Accept Queue)**。



**SYN Flood 攻击原理**

SYN Flood 属于典型的 DoS/DDoS 攻击。其攻击的原理很简单，就是用客户端在短时间内伪造大量不存在的 IP 地址，并向服务端疯狂发送 `SYN`。对于服务端而言，会产生两个危险的后果:

1. 处理大量的`SYN`包并返回对应`ACK`, 势必有大量连接处于`SYN_RCVD`状态，从而占满整个**半连接队列**，无法处理正常的请求。
2. 由于是不存在的 IP，服务端长时间收不到客户端的`ACK`，会导致服务端不断重发数据，直到耗尽服务端的资源。



**如何应对 SYN Flood 攻击？**

1. 增加 SYN 连接，也就是增加半连接队列的容量。
2. 减少 SYN + ACK 重试次数，避免大量的超时重发。
3. 利用 SYN Cookie 技术，在服务端接收到 `SYN`后不立即分配连接资源，而是根据这个 `SYN`计算出一个Cookie，连同第二次握手回复给客户端，在客户端回复 `ACK`的时候带上这个 `Cookie`值，服务端验证 Cookie 合法之后才分配连接资源。















### HTTP

#### 基本知识

​		HTTP（HyperText Transfer Protocol，超文本传输协议）是一种用于分布式、协作式和超媒体信息系统的应用层协议。HTTP 是万维网的数据通信的基础。

​		早在 HTTP 建立之初，主要就是为了将超文本标记语言(HTML)文档从Web服务器传送到客户端的浏览器。也是说对于前端来说，我们所写的HTML页面将要放在我们的 web 服务器上，用户端通过浏览器访问url地址来获取网页的显示内容，但是到了 WEB2.0 以来，我们的页面变得复杂，不仅仅单纯的是一些简单的文字和图片，同时我们的 HTML 页面有了 CSS，Javascript，来丰富我们的页面展示，当 ajax 的出现，我们又多了一种向服务器端获取数据的方法，这些其实都是基于HTTP 协议的。





#### HTTP 优化

影响一个 HTTP 网络请求的因素主要有两个：**带宽和延迟。**

- **带宽：**如果说我们还停留在拨号上网的阶段，带宽可能会成为一个比较严重影响请求的问题，但是现在网络基础建设已经使得带宽得到极大的提升，我们不再会担心由带宽而影响网速，那么就只剩下延迟了。

- **延迟：**

- - 浏览器阻塞（HOL blocking）：浏览器会因为一些原因阻塞请求。浏览器对于同一个域名，同时只能有 4 个连接（这个根据浏览器内核不同可能会有所差异），超过浏览器最大连接数限制，后续请求就会被阻塞。
  - DNS 查询（DNS Lookup）：浏览器需要知道目标服务器的 IP 才能建立连接。将域名解析为 IP 的这个系统就是 DNS。这个通常可以利用DNS缓存结果来达到减少这个时间的目的。
  - 建立连接（Initial connection）：HTTP 是基于 TCP 协议的，浏览器最快也要在第三次握手时才能捎带 HTTP 请求报文，达到真正的建立连接，但是这些连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大。





#### HTTP 1.0 和 HTTP 1.1

HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在：

1.  **缓存处理**，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。
2. **带宽优化及网络连接的使用**，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。
3. **错误通知的管理**，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。
4. **Host头处理**，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。
5. **长连接**，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。





#### HTTPS 与 HTTP 的区别

![img](https://www.runoob.com/wp-content/uploads/2018/09/HTTP-vs-HTTPS.png)

**HTTP** 协议以明文方式发送内容，**不提供任何方式的数据加密**，**不验证身份，导致身份可能被伪装** ，**也不会验证数据的前后一致性**，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。

**HTTPS**（Hypertext Transfer Protocol Secure：超文本传输安全协议）是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。

![img](https://www.runoob.com/wp-content/uploads/2018/09/https-intro.png)

- HTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。
- 使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。
- HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 SSL 握手需要的 9 个包，所以一共是 12 个包。
- http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。
- HTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。

















































## C++

### 内存管理

#### 内存布局

**c++ 程序的内存分布**

<img src="http://blog-10039692.file.myqcloud.com/1503627445050_5296_1503627445162.png" alt="C/C++ 学习笔记七（内存管理） - 云+社区- 腾讯云" style="zoom: 33%;" />



**函数栈**

栈帧是指为一个函数调用单独分配的那部分栈空间。比如，当运行中的程序调用另一个函数时，就要进入一个新的栈帧，原来函数的栈帧称为调用者的帧，新的栈帧称为当前帧。**被调用的函数运行结束后当前帧全部收缩，回到调用者的帧**。`%ebp`是帧指针，它总是指向当前帧的底部；`%esp`是栈指针，它总是指向当前帧的顶部。

堆栈帧包含如下数据:

*  函数返回地址

* 局部变量/CPU寄存器数据备份

![img](https://pic3.zhimg.com/80/v2-869540b280be6f354556c41dc28c4d56_1440w.jpg)







#### 内存对齐

对于基础类型，如float, double, int, char等，它们的大小和内存占用是一致的。而对于结构体而言，如果我们取得其sizeof的结果，会发现这个值有可能会大于结构体内所有成员大小的总和，这是由于**结构体内部成员进行了内存对齐**。

**内存对齐的规则**

定义有效对齐值（alignment）为结构体中 最宽成员 和 编译器/用户指定对齐值 中较小的那个。

(1) 结构体起始地址为有效对齐值的整数倍

(2) 结构体总大小为有效对齐值的整数倍

(3) 结构体第一个成员偏移值为0，之后成员的偏移值为 min(有效对齐值, 自身大小) 的整数倍

相当于每个成员要进行对齐，并且整个结构体也需要进行对齐。





#### **内存碎片**

程序的内存往往不是紧凑连续排布的，而是存在着许多碎片。我们根据碎片产生的原因把碎片分为内部碎片和外部碎片两种类型：

(1) **内部碎片**：系统分配的内存大于实际所需的内存（由于对齐机制）；

(2) **外部碎片**：不断分配回收不同大小的内存，由于内存分布散乱，较大内存无法分配；







### 内存模型

#### 为什么需要内存模型

​		在 C++11标准出来之前，C++环境没有多线程的概念。编译器和处理器认为系统中只有一个执行流。引入了多线程之后，情况就会变得非常复杂。这是因为：现代计算机系统为了加快执行效率，自动的包含了很多的优化。这些优化虽然保证了在单线程环境下不破坏原来的逻辑。但是一旦到了多线程之后，情况就不一样了。

之所以会产生差异，原因主要来自下面三个方面：

- **编译器优化**
- **CPU out-of-order执行**
- **CPU Cache不一致性**







#### Memory Reorder

以下面这段伪代码为例：

```c++
X = 0, Y = 0;

Thread 1: 
    X = 1; // ①
    r1 = Y; // ②

Thread 2: 
    Y = 1;
    r2 = X;
```

你可能会觉得，在这个程序执行完成之后，`r1` 和 `r2`怎么都不可能同时为0。但事实[并非如此](https://preshing.com/20120515/memory-reordering-caught-in-the-act/)。

这是因为“Memory Reorder”的存在，“Memory Reorder” 包含了**编译器和处理器两种类型的乱序**。

<img src="https://paul-pub.oss-cn-beijing.aliyuncs.com/2019/2019-12-05-cpp-memory-model/memory-reorder.png" alt="img" style="zoom:50%;" />



这就导致：线程1中事件发生的顺序虽然是先①后②，但是对于线程2来说，它看到结果可能却是先②后①。甚至，**当今的所有硬件平台，没有任何一个会提供完全的顺序一致（sequentially consistent）内存模型**，因为这样做效率太低了。

不同的编译器和处理器对于Memory Reorder有不同的偏好，但它们都遵循一定的原则，那就是：**不能修改单线程的行为**（[Thou shalt not modify the behavior of a single-threaded program.](https://preshing.com/20120625/memory-ordering-at-compile-time/)）。在这个基础上，它们可以做各种类型的优化。



**编译器优化**

编译器只要保证：在单线程环境下，执行的结果和原先一样就可以了。

对于编译器来说，它知道的是：当前线程中，数据的读写以及数据之间的依赖关系。但是，**编译器并不知道哪些数据是在线程间共享，而且是有可能会被修改的**。这就需要开发者在软件层面做好控制。对于编译器的乱序优化来说，开发者并非完全不能控制。编译器会提供称之为[内存栅栏（Memory Barrier）](https://stackoverflow.com/questions/286629/what-is-a-memory-fence)的工具给开发者，让开发者告诉编译器：这部分代码编译的时候不能乱序。



**Out-of-order执行**

不仅仅是编译器，处理器也可能会乱序执行指令。不同架构的CPU会有不同类型的Memory Reorder偏好。

我们使用的台式机和笔记本电脑基本上都是x86架构的CPU，而手机或者平板之类的移动设备一般用的是ARM架构的CPU。相较而言，前者的乱序类型要比后者少很多。

下面这幅图是[Preshing on Programming](https://preshing.com/)一篇文章中给出的对比关系图。

![img](https://paul-pub.oss-cn-beijing.aliyuncs.com/2019/2019-12-05-cpp-memory-model/weak-strong-table.png)

由此我们可以推算，在多线程环境下，假设我们写的代码包含了未定义行为，那么这些问题在手机上将比在电脑上更容易暴露出来。



**Cache Coherency**

事情还不只这么简单。现代的主流CPU几乎都会包含多个核以及多级Cache。

<img src="https://paul-pub.oss-cn-beijing.aliyuncs.com/2019/2019-12-05-cpp-memory-model/cache.png" alt="img" style="zoom: 33%;" />

每个**CPU核在运行**的时候，都会优先考虑**离自己最近的Cache**，一旦命中就直接使用Cache中的数据。这是因为Cache相较于主存（RAM）来说要快很多。

但是每个核之间的Cache，每一层之间的Cache，数据常常是不一致的。而同步这些数据是需要消耗时间的。这就会造成一个问题：某个CPU核修改了一个数据，没有同步的让其他核知道，于是就存在了数据不一致的情况。

综上这些原因让我们知道，CPU所运行的程序和我们编写的代码可能是不一致的。甚至，对于同一次执行，不同线程感知到其他线程的执行顺序可能都是不一样的。因此内存模型需要考虑到所有这些细节，以便让开发者可以精确控制。因为所有未定义的行为都可能产生问题。







#### 对象和内存位置

我们已经知道，C++中的数据都是由对象组成。一个对象包含了若干个内存位置。

每个对象从初始化开始，直到最终销毁，在其生命周期的范围内，对它进行的访问必须有一个确定的修改顺序，这个顺序包含了所有线程的访问操作。虽然程序的每一次运行，这个顺序可能是不一样的（例如：CPU资源的变化，调度器的影响），但是针对其中具体的某一次来说，必须有一个“一致的顺序”，这个顺序要被所有的线程认可，并且可见。

例如：一旦某个线程修改了一个数据，这个操作必须要让所有线程知道，在修改操作之后，所有线程都应该得到修改后的值。

从数据类型的角度来说，有两种情况：

- 对于原子类型（见下文）：由编译器保证数据的同步。
- 对于非原子类型：由开发者保证。

并发编程的难点之一就在于：识别出系统中那些在线程共享且可能会被修改的数据，并对它们做“合理”的保护。之所以强调这一点，是因为对于共享数据的保护本质上是在对抗编译器和处理器的优化，所以保护不能过度（在讲解并发编程的时候我们提到了锁的粒度）。

我们必须在保证正确性的基础上尽可能少的干扰编译器和处理器的优化：对于那些没有访问共享数据，或者对于所有线程来说都是只读的数据来说，这部分代码就任由编译器和处理器优化好了。

另外还有一点需要说明的是，这里说的是：对于**每一个**变量来说，要有明确的修改顺序。但是这并不要求所有的变量存在一个全局的一致顺序。这意味着，当将多个变量的访问顺序放在一起看的时候，不同线程看到的顺序可能是不一样的。







#### 具体的内存模型

* seq-cst 模型
* acq-rel 模型
* relaxed 模型

详见 https://paul.pub/cpp-memory-model/













### 静态链接和动态链接

在我们的实际开发中，不可能将所有代码放在一个源文件中，所以会出现**多个源文件**，而且多个源文件之间不是独立的，而会存在多种**依赖关系**，如一个源文件可能要调用另一个源文件中定义的函数。**每个源文件都是独立编译的**，即每个*.c文件会形成一个*.o文件。为了满足前面说的依赖关系，则需要将这些源文件产生的目标文件进行链接，从而形成一个可以执行的程序。这个过程就是**静态链接**

**动态链接**的基本思想是**把程序按照模块拆分成各个相对独立部分**，在**程序运行**时才将它们链接在一起形成一个完整的程序，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文件。



#### 静态链接的优缺点

​        静态链接的缺点很明显，一是浪费空间，因为每个可执行程序中对所有需要的目标文件都要有一份副本，所以如果多个程序对同一个目标文件都有依赖，如多个程序中都调用了printf()函数，则这多个程序中都含有printf.o，所以同一个目标文件都在内存存在多个副本；另一方面就是更新比较困难，因为每当库函数的代码修改了，这个时候就需要重新进行编译链接形成可执行程序。但是静态链接的优点就是，在可执行程序中已经具备了所有执行程序所需要的任何东西，在执行的时候运行速度快。



#### 动态链接的过程

​		假设现在有两个程序program1.o和program2.o，这两者共用同一个库lib.o,假设首先运行程序program1，系统首先加载program1.o，当系统发现program1.o中用到了lib.o，即program1.o依赖于lib.o，那么系统接着加载lib.o，如果program1.o和lib.o还依赖于其他目标文件，则依次全部加载到内存中。当program2运行时，同样的加载program2.o，然后发现program2.o依赖于lib.o，但是此时lib.o已经存在于内存中，这个时候就不再进行重新加载，而是将内存中已经存在的lib.o映射到program2的虚拟地址空间中，从而进行链接（这个链接过程和静态链接类似）形成可执行程序。



#### 动态链接的优缺点

​		动态链接的优点显而易见，就是即使需要每个程序都依赖同一个库，但是该库不会像静态链接那样在内存中存在多份副本，而是这多个程序在执行时**共享同一份副本**；

​		另一个优点是，更新也比较方便，**更新时只需要替换原来的目标文件**，而无需将所有的程序再重新链接一遍。当程序下一次运行时，新版本的目标文件会被自动加载到内存并且链接起来，程序就完成了升级的目标。但是动态链接也是有缺点的，因为把**链接推迟到了程序运行时**，所以每次执行程序都需要进行链接，所以**性能会有一定损失**。

















### 函数参数压栈顺序

**参数从右到左入栈**

对于 printf（const char* format, …） 这样的**不定参函数**，编译器通过format中的%占位符的个数来确定参数的个数。 

假设参数的压栈顺序是从左到右的，这时，函数调用的时候，format最先进栈，之后是各个参数进栈，最后 pc 进栈，此时，由于format先进栈了，上面压着未知个数的参数，想要知道参数的个数，必须找到format，而要找到format，必须要知道参数的个数，这样就陷入了一个无法求解的死循环了！！

而如果把参数从右到左压栈，函数调用时，先把若干个参数都压入栈中，再压format，最后压pc，这样一来，栈顶指针加 2 便找到了format，通过format中的%占位符，取得后面参数的个数，从而正确取得所有参数。 

所以，如果不存这种不定参的函数，则参数的压栈顺序无论是从左到右还是从右到左都是没关系的









































## 其他

### 内存池，进程池，线程池

#### 池的概念

由于服务器的硬件资源“充裕”，那么**提高服务器性能**的一个很直接的方法就是以**空间换时间**，即“浪费”服务器的硬件资源，以换取其运行效率。这就是池的概念。

池是一组资源的集合，这组资源在**服务器启动之初就完全被创建并初始化，这称为静态资源分配**。当服务器进入正式运行阶段，即开始处理客户请求的时候，如果它需要相关的资源，就可以直接从池中获取，无需动态分配。

很显然，直接**从池中取得所需资源比动态分配资源的速度要快得多**，因为分配系统资源的系统调用都是很耗时的。**当服务器处理完一个客户连接后，可以把相关的资源放回池中，无需执行系统调用来释放资源**。

从最终效果来看，池相当于服务器管理系统资源的应用设施，它避免了服务器对内核的频繁访问。







#### 内存池

内存池是一种内存分配方式。

通常我们习惯直接使用new、malloc等**系统调用申请分配内存**，这样做的缺点在于：由于所申请内存块的大小不定，当**频繁使用时会造成大量的内存碎片并进而降低性能**。

内存池则是**在真正使用内存之前，先申请分配一定数量的、大小相等(一般情况下)的内存块留作备用**。当有新的内存需求时，就从内存池中分出一部分内存块，若内存块不够再继续申请新的内存。这样做的一个显著优点是，使得内存分配效率得到提升。







#### 进程池和线程池

进程池和线程池相似，所以这里我们以进程池为例进行介绍。

进程池是由服务器预先创建的一组子进程，这些子进程的数目一般在 3~10 个之间。线程池中的线程数量应该和 CPU 数量差不多。

进程池中的所有子进程都运行着相同的代码，并具有相同的属性，比如优先级、 PID 等。

当有新的任务来到时，主进程将通过某种方式选择进程池中的某一个子进程来为之服务。相比于动态创建子进程，选择一个已经存在的子进程的代价显得小得多。至于主进程选择哪个子进程来为新任务服务，则有两种方法：

1）**主进程使用某种算法来主动选择子进程**。最简单、最常用的算法是随机算法和 Round Robin （轮流算法）。

2）**主进程和所有子进程通过一个共享的工作队列来同步**，子进程都睡眠在该工作队列上。当有新的任务到来时，主进程将任务添加到工作队列中。这将唤醒正在等待任务的子进程，不过只有一个子进程将获得新任务的“接管权”，它可以从工作队列中取出任务并执行之，而其他子进程将继续睡眠在工作队列上。

**当选择好子进程后，主进程还需要使用某种通知机制来告诉目标子进程有新任务需要处理，并传递必要的数据**。最简单的方式是，在父进程和子进程之间预先建立好一条管道，然后通过管道来实现所有的进程间通信。在父线程和子线程之间传递数据就要简单得多，因为我们可以把这些数据定义为全局，那么它们本身就是被所有线程共享的。

![线程池结构](http://wangpengcheng.github.io/img/thread_pool.png)

线程池的主要组成有上面三个部分：

- 任务队列(Task Quene)
- 线程池(Thread Pool)
- 完成队列(Completed Tasks)



实现线程池有什么好处呢？

- 降低资源消耗：池化技术可以重复利用已经创建的线程，降低线程创建和销毁的损耗。
- 提高响应速度：利用已经存在的线程进行处理，少去了创建线程的时间
- 管理线程可控：线程是稀缺资源，不能无限创建，线程池可以做到统一分配和监控
- 拓展其他功能：比如定时线程池，可以定时执行任务



线程池主要用于：

* **需要大量的线程来完成任务，且完成任务的时间比较短**。 比如WEB服务器完成网页请求这样的任务，使用线程池技术是非常合适的。因为单个任务小，而任务数量巨大。但对于长时间的任务，比如一个Telnet连接请求，线程池的优点就不明显了。因为Telnet会话时间比线程的创建时间大多了。
* 对性能要求苛刻的应用，比如要求服务器迅速响应客户请求。
* 接受突发性的大量请求，但不至于使服务器因此产生大量线程的应用。

